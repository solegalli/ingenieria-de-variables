{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización de las variables\n",
    "\n",
    "Discutimos previamente que la magnitud or rango de valores en una variables es una consideración importante cuando estamos construyendo modelos de machine learning. Brevemente:\n",
    "\n",
    "\n",
    "### La magnitud de las variables importa porque:\n",
    "\n",
    "- Los coeficientes de regresión de los modelos lineales estan influenciados directamente por la magnitud de las variables\n",
    "- Las variables con mayor escala / mayor rango de valores dominan sobre aquellas con escala más pequeña / menor rango de valores .\n",
    "- El algoritmo del Gradiente Descendente converge más rápido cuando las variables tienen rangos de valores similares.\n",
    "- El escalamiento de las variables ayuda a reducir el tiempo para encontrar los vectores de soporte en las Máquinas de vectores de soporte (SVMs)\n",
    "- La distancia euclidiana son sensibles a la escala de las variables\n",
    "- Algunos algoritmos, como el Análisis de Componentes Principales - PCA  requieren que las variables esten centradas alrededor de 0.\n",
    "\n",
    "\n",
    "### Los modelos de machine learning que se ven afectados por la escala de las variables son:\n",
    "\n",
    "- Regresión Lineal y Logística \n",
    "- Redes Neuronales\n",
    "- Máquinas de vectores de soporte (SVMs)\n",
    "- K vecinos más cercanos (KNN)\n",
    "- Agrupamiento K-medias\n",
    "- Análisis discriminante lineal (LDA)\n",
    "- Análisis de Componentes Principales (PCA)\n",
    "\n",
    "\n",
    "### Normalización de las variables\n",
    "\n",
    "Discutimos previamente que para que funcionen mejor muchos algoritmos de Machine Learning usados en Data Science, hay que normalizar las variables de entrada al algoritmo, es decir, **antes del entrenamiento del modelo de aprendizaje de máquina (machine learning)**.\n",
    "\n",
    "**Normalizar** significa, en este caso, comprimir o extender los valores de la variable para que estén en un rango definido. \n",
    "\n",
    "Hay varios métodos de normalización , las cuales discutiremos en esta sección: \n",
    "\n",
    "\n",
    "- Escalado estándar (Standard Scaler)\n",
    "- Normalización promedio \n",
    "- Escalado valores mínimumo y máximo - MinMaxScaling\n",
    "- Escalado valor máximo - MaxAbsScaling\n",
    "- Escalado cuantiles y mediana - RobustScaling\n",
    "- Normalización vector de unidad\n",
    "\n",
    "\n",
    "En este notebook discutiremos  **Escalado estándar**.\n",
    "\n",
    "=================================================================\n",
    "\n",
    "## Escalado estándar\n",
    "\n",
    "En la técnica conocida como escalado estándar, a cada dato se le resta la media de la variable y se le divide por la desviación estándar. Con esta ténnica los datos quedan centrados alredor de zero y su varianza es igual a 1. \n",
    "\n",
    "**z = (x - x_media) /  std**\n",
    "\n",
    "El resultado de la transformación anterior es  **z**, conocido como los puntaje z ( ó z-score en inglés) , y estos representan la distancia de cada observación con respecto a la media, expresándolas en unidades de desviación estándar.\n",
    "\n",
    "Un puntaje z especifica la posición de la observación en una distribución (en unidades de desviación estándar) respecto a la media de la distribución. El signo de el puntaje z (+ ó - ) indican si la observación está por encima (+) o por debajo ( - ) de la media.\n",
    "\n",
    "La forma de una distribución estandarizada  (o normalizada a su puntaje z) es idéntica que la distribución de la variable original. Si la distribución de la variable es normal, la distribución estandarizada será normal. Pero si la distribución original está sesgada, la distribución estandarizada también estará sesgada. En otras palabras,  **estandarizando una variable no normaliza la distribución de los datos** y si este es el resultado deseado, debemos implementar alguna de las técnias que discutimos en la sección 7 de este curso.\n",
    "\n",
    "Para resumir, el escalado estándar:\n",
    "\n",
    "- centraliza la media alrededor de 0\n",
    "- escala la varianza de la variable a la unidad\n",
    "- prepresva la forma original de la distribución \n",
    "- los valores mínimos y máximos de la variable varían.\n",
    "- preserva los valores extremos\n",
    "\n",
    "Este tipo de escalamiento es apropiada para los algoritmos que requiren las variables centradas alrededor de cero.\n",
    "\n",
    "## En este demo\n",
    "\n",
    "Aprenderás como estandarizar variables utilizando los datos del Boston House Prices disponible en Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# conjunto de datos para el demo\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# clase para estandarización \n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar datos del Boston House price de sklearn\n",
    "boston_dataset = load_boston()\n",
    "\n",
    "# crea un dataframe con las variables independientes\n",
    "data = pd.DataFrame(boston_dataset.data,\n",
    "                      columns=boston_dataset.feature_names)\n",
    "\n",
    "# añadir la variable target\n",
    "data['MEDV'] = boston_dataset.target\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información acerca del conjunto de datos\n",
    "# Boston House Prices: Precios de casas en Boston\n",
    "\n",
    "# El objectivo es predecir \n",
    "# \"el valor mediano de las casas en Boston\" \n",
    "# esta es la columna MEDV en los datos\n",
    "\n",
    "# el resto de las variables representan características\n",
    "# acerca de las casas y de los vencidarios\n",
    "\n",
    "#imprimir la descripción de los datos\n",
    "print(boston_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miremos los pamámetros estadísticos principales de cada\n",
    "# variable para darnos una idea del rango de valores\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferentes variables tienen diferentes rangos de valores representados por la media (mean), max, min y desviación estandar, etc. En otras palabras, tienen diferentes magnitudes o escalas. Pon atención como en este demo,   **los valores promedio no están centrados alrededor de zero y la desviación estadar no esta escalada al valor unitario**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando estandarizamos el conjunto de datos, primero necesitamos indentificar la media o la desviación estándar de las variables. Estos parámetros necesitan ser aprendidos en el set de entrenamiento, guardados y luego usados para escalar el set de prueba y cualquier datos futuros. Por lo tanto, primero dividiremos los datos en los sets de entrenamiento y prueba, como hemos hecho a lo largo del curso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separaemos los datos en los sets de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('MEDV', axis=1),\n",
    "                                                    data['MEDV'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estandarización\n",
    "\n",
    "La clase StandardScaler de scikit-learn estándariza los datos eliminando la media y escalándolos de forma que su varianza sea igual a 1.  Además, aprende y guarda los parámetros que se necesitan para la estandarización. Por lo tanto, esta clase es una excelente selección cuando queremos aplicar esta ténica.\n",
    "\n",
    "Una de las desventajas es que no puedes seleccionar directamente cual variable escalar; de hecho, escala todas las variables en los datos, y returna un arreglo NumPy, sin los nombres de las variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarización: con el StandardScaler de sklearn\n",
    "\n",
    "# inicializa el escalador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# entrena el escalador en el set de entrenamiento\n",
    "# aprende los parámetros \n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transforma los sets de entrenamiento y prueba \n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el escalador guarda la media de las variables\n",
    "# aprendidas del set de entrenamiento\n",
    "\n",
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el escalador guarda la desviación estándar\n",
    "# de las variables aprendidas del set de entrenamiento\n",
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora transformemos los arreglos NumPy resultantes \n",
    "# en dataframes para el resto del demo\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miremos el set de entrenamiento original:\n",
    "# media y desviación estándar \n",
    "# Aqui usamos np.round para reducir el número \n",
    "# de decimalaes a 1 \n",
    "np.round(X_train.describe(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miremos el set de entrenamiento normalizado:\n",
    "# media y desviación estándar \n",
    "# Aqui usamos np.round para reducir el número \n",
    "# de decimalaes a 1 \n",
    "\n",
    "np.round(X_train_scaled.describe(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de esperarse, la media de cada variable, que originalmente no estaban centradas alrededor de zero, ahora si lo están y la desviación estandar es 1.\n",
    "Fíjate, que sin embargo, los valores mínimos y máximos varían de acuerdo al rango de valores originales en las variables y es altamente influenciado por la presencia de valores extremos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparemos la distribución de las variables \n",
    "# antes y despues del escalamiento\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# antes de la normalización\n",
    "ax1.set_title('Antes del escalamiento')\n",
    "sns.kdeplot(X_train['RM'], ax=ax1)\n",
    "sns.kdeplot(X_train['LSTAT'], ax=ax1)\n",
    "sns.kdeplot(X_train['CRIM'], ax=ax1)\n",
    "\n",
    "# después de la normalización\n",
    "ax2.set_title('Después del escalamiento estándar')\n",
    "sns.kdeplot(X_train_scaled['RM'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['LSTAT'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['CRIM'], ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fíjate en las gráficas anteriores como la estandarización centra las distribuciones alrededor de zero, pero preserva la distribución original. El rango de valores no es idéntico, pero es más o menos homogéneo en las variables. \n",
    "\n",
    "Ahora, pon atención a las gráficas siguientes, algo interesante sucede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparemos la distribución de las variables \n",
    "# antes y despues del escalamiento\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# antes de la normalización\n",
    "ax1.set_title('Antes del escalamiento')\n",
    "sns.kdeplot(X_train['AGE'], ax=ax1)\n",
    "sns.kdeplot(X_train['DIS'], ax=ax1)\n",
    "sns.kdeplot(X_train['NOX'], ax=ax1)\n",
    "\n",
    "\n",
    "# después de la normalización\n",
    "ax2.set_title('Después del escalamiento estándar')\n",
    "sns.kdeplot(X_train_scaled['AGE'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['DIS'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['NOX'], ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['AGE'].min(), X_train['AGE'].max(), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los gráficos anteriores, podemos ver, que al normalizar, la variable NOX, que tenía una escala de valores pequeña [0-1], y la variable AGE que tenía valores entre [0-100], ahora  están distribuidas en un rango más homogéneo de valores. De esta manera, ahora  podemos compararlas directamente en una gráfica, mientras que antes hubiése sido muy difícil. En un modelo lineal, AGE dominaría el resultado, pero luego luego de la normalización, ambas variables tienen la posiblidad de influenciar el resultado (asumiendo que ambas tienen poder predictivo).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train['AGE'], X_train['NOX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train_scaled['AGE'], X_train_scaled['NOX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feml",
   "language": "python",
   "name": "feml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
