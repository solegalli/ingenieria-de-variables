{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escala de variables\n",
    "\n",
    "### Que tan importante es la magnitud de una variable?\n",
    "\n",
    "En modelos de regresión lineal, la escala o magnitud de una variable es importante. Los modelos lineales de la forma **y = w x + b**, donde el coeficiente de regresion w representa el cambio esperado en y por una unidad de cambio in x ( el predictor). Por lo tanto, la magnitud de w es parcialmente determinada por la magniuded de las unidades en x. Si x es una variable de distancia, simplemente cambiando la escala de kilometros a millas causara un cambio en la magnitud del coeficiente. \n",
    "\n",
    "Adicionalmente, en situaciones donde se busca estimar y utilizando multiples predictores x1, x2, ...xn, los predictores la escala de valores mas grande dominarán sobre aquellos que tengan un rango más pequeño.\n",
    "\n",
    "Por otro lado, el algoritmo del descenso de gradiente converge más rápido cuando todos los predictores (x1 a xn) tienen una escala de valores similar, por lo tanto tener variables con magnitudes similares es tambien bastante util para las Redes Neuronales.\n",
    "\n",
    "Con las Máquinas de Soporte Vectorial o Máquinas de Vectores de Soporte (Support Vector Machines SVM), la normalización de la escala de las variables puede reducir el tiempo requerido para encontrar los vectores de soporte. \n",
    "\n",
    "Finalmente, métodos que usan Distancias Euclidianas o distancias en general, tambien son sensibles a las variaciones en la magnitud o escala de los predictores, ya que intrinsicamente afectan el cálculo de las distancias. Por lo tanto, el escalamiento de las variables es necesario para métodos que utilicen distancias como   K-vecinos más cercanos o K-NN (K Nearest Neighbours) y segmentos   k-means.\n",
    "\n",
    "En resumen:\n",
    "\n",
    "#### La magnitud o escala de las variables es importance porque:\n",
    "\n",
    "- El coeficiente de regresión es directamente influenciado por la escala de la variable.\n",
    "- Variables con mayor magnitud/rango de valores dominan sobre aquellas con un rango menor.\n",
    "- El algoritmo del descenso de gradiente converge cuando las variables tienen escalas similares.\n",
    "- El escalamiento de las variables ayuda a reducir el tiempo para encontrar los vectores soporte para SVMs.\n",
    "- Medidas de distancia como la Euclidiana son sensibles a las escalas de magnitud de las variables.\n",
    "\n",
    "#### Los modelos de machine learning que se afectan más por la escala o magnitud de las variables son:\n",
    "\n",
    "- Regresiones Lineales y Logísticas\n",
    "- Redes Neuronales \n",
    "- Máquinas de Soporte Vectorial\n",
    "- KNN\n",
    "- Agrupación K-means \n",
    "- Análisis Discriminante Lineal (LDA)\n",
    "- Análisis Principales Componenentes (PCA)\n",
    "\n",
    "#### Los modelos de machine learning menos sensibles a  la escala o magnitud de las variables son aquellos basados en árboles:\n",
    "\n",
    "- Árboles de Clasificación y Regresión. \n",
    "- Bosques Aleatorios (Random Forest) \n",
    "- Árboles de Potenciación del gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================\n",
    "\n",
    "## En este Demo\n",
    "\n",
    "Vamos a estudiar el efector de la magnitud de las variables en el desempeño de los diferentes modelos de machine learning.\n",
    "\n",
    "Usaremos los datos del Titanic.\n",
    "\n",
    "- Para descargar los datos por favor referise a la lección **Datasets** en la **Sección 1** del curso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# importar varios modelos de machine learning \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# para escalar las variables\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# para evaluar el desempeño y separar datos en entrenamiento y prueba\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar los datos con las variables numéricas únicamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>211.3375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  survived      age      fare\n",
       "0       1         1  29.0000  211.3375\n",
       "1       1         1   0.9167  151.5500\n",
       "2       1         0   2.0000  151.5500\n",
       "3       1         0  30.0000  151.5500\n",
       "4       1         0  25.0000  151.5500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar las variables numéricas del Titanic\n",
    "\n",
    "data = pd.read_csv('../titanic.csv',\n",
    "                   usecols=['pclass', 'age', 'fare', 'survived'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1046.000000</td>\n",
       "      <td>1308.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.294882</td>\n",
       "      <td>0.381971</td>\n",
       "      <td>29.881135</td>\n",
       "      <td>33.295479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.837836</td>\n",
       "      <td>0.486055</td>\n",
       "      <td>14.413500</td>\n",
       "      <td>51.758668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>7.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>31.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pclass     survived          age         fare\n",
       "count  1309.000000  1309.000000  1046.000000  1308.000000\n",
       "mean      2.294882     0.381971    29.881135    33.295479\n",
       "std       0.837836     0.486055    14.413500    51.758668\n",
       "min       1.000000     0.000000     0.166700     0.000000\n",
       "25%       2.000000     0.000000    21.000000     7.895800\n",
       "50%       3.000000     0.000000    28.000000    14.454200\n",
       "75%       3.000000     1.000000    39.000000    31.275000\n",
       "max       3.000000     1.000000    80.000000   512.329200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# miremos los valores de cada una de las variables\n",
    "# para darnos una idea de la mgnitud/escala de las variables\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que la variable fare varia entre 0 y 512, age entre 0 y 80, y class entre 0 y 3. Las variables tienen claramente diferentes escalas de magnitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rango: pclass 2\n",
      "rango: age 79.8333\n",
      "rango: fare 512.3292\n"
     ]
    }
   ],
   "source": [
    "# calculemos el rango\n",
    "\n",
    "for col in ['pclass', 'age', 'fare']:\n",
    "    print('rango:',col, data[col].max() - data[col].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El rango de valores que puede tomar cada una de las variables es diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((916, 3), (393, 3))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  separemos los datos en entrenamiento y prueba\n",
    "# los datos del titanic contienen datos ausentes\n",
    "# para este demo, los sustituiremos con 0s.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[['pclass', 'age', 'fare']].fillna(0),\n",
    "    data.survived,\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalamiento de las variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este demo, normalizaremos las variables entre 0 y 1, usando  el MinMaxScaler de scikit-learn. Para aprender más acerca de este escalamiento por favor visita la página de internet de Scikit-Learn [website](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "\n",
    "La transformación: \n",
    "\n",
    "X_rescaled = X - X.min() / (X.max - X.min()\n",
    "\n",
    "y para transformar de vuelta las variables normalizadas a su escala original:\n",
    "\n",
    "X = X_rescaled * (max - min) + min\n",
    "\n",
    "**Hay una sección dedicada al escalamiento de las variables más adelante en el curso, donde explicaremos esta y otras técnicas de normalización en detalle**. Por ahora, continuemos con el demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escalemos las variables entre 0 y 1.\n",
    "\n",
    "# scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# ajustar el scaler\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# re escaler los datos\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media:  [0.64628821 0.33048359 0.06349833]\n",
      "Desviación Estandard:  [0.42105785 0.23332045 0.09250036]\n",
      "Valor Mínimo:  [0. 0. 0.]\n",
      "Valor Máximo:  [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#vamos los datos normalizados\n",
    "\n",
    "print('Media: ', X_train_scaled.mean(axis=0))\n",
    "print('Desviación Estandard: ', X_train_scaled.std(axis=0))\n",
    "print('Valor Mínimo: ', X_train_scaled.min(axis=0))\n",
    "print('Valor Máximo: ', X_train_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, los valores máximos de todas las variables es 1, y el valor mínimo es 0, como era de esperarse después de la normalización. Ahora todas las variables tienen una variable similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Logística \n",
    "\n",
    "Evaluemos el efecto del escalamiento de las variables en una regresión logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmento de entrenamiento\n",
      "Regresión logística roc-auc: 0.6793181006244372\n",
      "Segmento de prueba\n",
      "Regresión logística roc-auc: 0.7175488081411426\n"
     ]
    }
   ],
   "source": [
    "# modelo construido con variables sin escalar\n",
    "\n",
    "# Definición del Modelo\n",
    "logit = LogisticRegression(\n",
    "    random_state=44,\n",
    "    C=1000,  # valor alto para prevenir regularización\n",
    "    solver='lbfgs')\n",
    "\n",
    "# entrenamiento del modelo \n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "# evaluación del desempeño \n",
    "print('Segmento de entrenamiento')\n",
    "pred = logit.predict_proba(X_train)\n",
    "print('Regresión logística roc-auc: {}'.format(\n",
    "    roc_auc_score(y_train, pred[:, 1])))\n",
    "print('Segmento de prueba')\n",
    "pred = logit.predict_proba(X_test)\n",
    "print('Regresión logística roc-auc: {}'.format(\n",
    "    roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.71428242, -0.00923013,  0.00425235]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# miremos los  coeficientes\n",
    "logit.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmento de entrenamiento\n",
      "Regresión logística roc-auc: 0.6475025032832007\n",
      "Segmento de prueba\n",
      "Regresión logística roc-auc: 0.7138570875504674\n",
      "Segmento de entrenamiento\n",
      "Regresión logística roc-auc: 0.6793281640744896\n",
      "Segmento de prueba\n",
      "Regresión logística roc-auc: 0.7175488081411426\n"
     ]
    }
   ],
   "source": [
    "# modelo construido con variables normalizadas\n",
    "\n",
    "# Definición del Modelo\n",
    "logit = LogisticRegression(\n",
    "    random_state=44,\n",
    "    C=1000,  # valor alto para prevenir regularización\n",
    "    solver='lbfgs')\n",
    "\n",
    "# entrenamiento del modelo con los datos re-escalados\n",
    "logit.fit(X_train_scaled, y_train)\n",
    "\n",
    "# evaluación del desempeño \n",
    "print('Segmento de entrenamiento')\n",
    "pred = logit.predict_proba(X_train)\n",
    "print('Regresión logística roc-auc: {}'.format(\n",
    "    roc_auc_score(y_train, pred[:, 1])))\n",
    "print('Segmento de prueba')\n",
    "pred = logit.predict_proba(X_test)\n",
    "print('Regresión logística roc-auc: {}'.format(\n",
    "    roc_auc_score(y_test, pred[:, 1])))\n",
    "\n",
    "\n",
    "# evaluación del desempeño \n",
    "print('Segmento de entrenamiento')\n",
    "pred = logit.predict_proba(X_train_scaled)\n",
    "print('Regresión logística roc-auc: {}'.format(\n",
    "    roc_auc_score(y_train, pred[:, 1])))\n",
    "print('Segmento de prueba')\n",
    "pred = logit.predict_proba(X_test_scaled)\n",
    "print('Regresión logística roc-auc: {}'.format(\n",
    "    roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.42875872, -0.68293349,  2.17646757]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el desempeño de la regresión logística no cambio cuando usamos los datos normalizados ( comparando el roc-auc para el segmento de prueba y entrenamiento para modelos con y sin variables escaladas).\n",
    "\n",
    "Sin embargo, cuando vemos los coeficientes vemos una gran diferencia en sus valores. Esto es porque la magnitud de las variables estaba afectando los coeficientes. Despues del escalamiento, todas las 3 variables tienen relativamente el mismo efecto (coeficientes) con respecto a 'survival', mientras que sin el escalamiento, estariamos más inclinados a pensar que pclass dominaba en la predicción de la tasa de supervivencia survival \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Máquinas de Soporte Vectorial SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmento de entrenamiento\n",
      "SVM roc-auc: 0.8824161337231242\n",
      "Segmento de prueba\n",
      "SVM roc-auc: 0.6618135058901611\n"
     ]
    }
   ],
   "source": [
    "# Modelo con variables sin normalizar\n",
    "\n",
    "# definición del modelo\n",
    "SVM_model = SVC(random_state=44, probability=True, gamma='auto')\n",
    "\n",
    "#  entrenamiento del modelo\n",
    "SVM_model.fit(X_train, y_train)\n",
    "\n",
    "# evaluación del desempeño\n",
    "print('Segmento de entrenamiento')\n",
    "pred = SVM_model.predict_proba(X_train)\n",
    "print('SVM roc-auc: {}'.format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print('Segmento de prueba')\n",
    "pred = SVM_model.predict_proba(X_test)\n",
    "print('SVM roc-auc: {}'.format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmento de entrenamiento\n",
      "SVM roc-auc: 0.6781306135182325\n",
      "Segmento de prueba\n",
      "SVM roc-auc: 0.6841159227918809\n"
     ]
    }
   ],
   "source": [
    "# Modelo con variables normalizadas\n",
    "\n",
    "# definición del modelo\n",
    "SVM_model = SVC(random_state=44, probability=True, gamma='auto')\n",
    "\n",
    "# entrenamiento del modelo\n",
    "SVM_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# evaluación del desempeño\n",
    "print('Segmento de entrenamiento')\n",
    "pred = SVM_model.predict_proba(X_train_scaled)\n",
    "print('SVM roc-auc: {}'.format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print('Segmento de prueba')\n",
    "pred = SVM_model.predict_proba(X_test_scaled)\n",
    "print('SVM roc-auc: {}'.format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El escalamiento de las variables mejoró el desempeño de SVMs . Luego de la normalización el modelo no está sobre-ajustando los datos de entrenamiento ( comparando el roc-auc 0.901 para el modelo sin escalar vs el roc-auc de 0.704 con normalización). Adicionamente, el roc-auc para el segmento de prueba se incrementó también (0.67 vs 0.69)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Neuronales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmento de entrenamiento\n",
      "Red Neuronal roc-auc: 0.6766814767106608\n",
      "Segmento de prueba\n",
      "Red Neuronal roc-auc: 0.7161246612466123\n"
     ]
    }
   ],
   "source": [
    "# Modelo con variables sin normalizar\n",
    "\n",
    "# definición del modelo\n",
    "NN_model = MLPClassifier(random_state=44, solver='sgd')\n",
    "\n",
    "# entrenamiento del modelo\n",
    "NN_model.fit(X_train, y_train)\n",
    "\n",
    "# evaluación de desempeño\n",
    "print('Segmento de entrenamiento')\n",
    "pred = NN_model.predict_proba(X_train)\n",
    "print('Red Neuronal roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Segmento de prueba')\n",
    "pred = NN_model.predict_proba(X_test)\n",
    "print('Red Neuronal roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmento de entrenamiento\n",
      "Red Neuronal roc-auc: 0.6754939896044562\n",
      "Segmento de prueba\n",
      "Red Neuronal roc-auc: 0.7080913666279519\n"
     ]
    }
   ],
   "source": [
    "# Modelo con variables normalizadas\n",
    "\n",
    "# definición del modelo\n",
    "NN_model = MLPClassifier(random_state=44, solver='sgd')\n",
    "\n",
    "# entrenamiento del model\n",
    "NN_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# evaluación de desempeño\n",
    "print('Segmento de entrenamiento')\n",
    "pred = NN_model.predict_proba(X_train_scaled)\n",
    "print('Red Neuronal roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Segmento de prueba')\n",
    "pred = NN_model.predict_proba(X_test_scaled)\n",
    "print('Red Neuronal roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El escalamiento de las variables mejoró el desempeño de las redes neuronales para el segmento de entranamiento y prueba (comparando  roc-auc para embos modelos). El roc-auc incremento en ambos segementos cuando el modelo se entrenó con las variables normalizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-vecinos cercanos (KNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmento de entrenamiento\n",
      "KNN roc-auc: 0.855810887646612\n",
      "Segmento de prueba\n",
      "KNN roc-auc: 0.6655467064874732\n"
     ]
    }
   ],
   "source": [
    "# Modelo con variables sin normalizar\n",
    "\n",
    "# definición del modelo\n",
    "KNN = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# entrenamiento del model\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "# evaluación de desempeño\n",
    "print('Segmento de entrenamiento')\n",
    "pred = KNN.predict_proba(X_train)\n",
    "print('KNN roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Segmento de prueba')\n",
    "pred = KNN.predict_proba(X_test)\n",
    "print('KNN roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmento de entrenamiento\n",
      "KNN roc-auc: 0.8600954015064985\n",
      "Segmento de prueba\n",
      "KNN roc-auc: 0.6998644986449865\n"
     ]
    }
   ],
   "source": [
    "# Modelo con variables normalizadas\n",
    "\n",
    "# definición del modelo\n",
    "KNN = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# entrenamiento del model\n",
    "KNN.fit(X_train_scaled, y_train)\n",
    "\n",
    "# evaluación de desempeño\n",
    "print('Segmento de entrenamiento')\n",
    "pred = KNN.predict_proba(X_train_scaled)\n",
    "print('KNN roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Segmento de prueba')\n",
    "pred = KNN.predict_proba(X_test_scaled)\n",
    "print('KNN roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que para KNN el escalamiento tambien ha mejorado el desempeño del modelo. El modelo construido con las variables normalizadas tiene una mejor generalización, ya que el roc-auc es mas alto en el segmento de prueba (0.70 vs 0.62 para el modelo sin escalamiento).\n",
    "\n",
    "Ambos modelos KNN estan sobre-ajustando el segmento de entrenamiento. Esto es un indicativo que es necesario cambiar los parámetros del modelo o utilizar menos variables para intentar mininzar el sobre-ajuste, pero esta por fuera del alcance de este demo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmento de entrenamiento\n",
      "Random Forests roc-auc: 0.9866810238554083\n",
      "Segmento de prueba\n",
      "Random Forests roc-auc: 0.7326751838946961\n"
     ]
    }
   ],
   "source": [
    "# Modelo con variables sin normalizar\n",
    "\n",
    "# definición del modelo\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=39)\n",
    "\n",
    "# entrenamiento del model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# evaluación de desempeño\n",
    "print('Segmento de entrenamiento')\n",
    "pred = rf.predict_proba(X_train)\n",
    "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print('Segmento de prueba')\n",
    "pred = rf.predict_proba(X_test)\n",
    "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmento de entrenamiento\n",
      "Random Forests roc-auc: 0.9867917218059866\n",
      "Segmento de prueba\n",
      "Random Forests roc-auc: 0.7312510370001659\n"
     ]
    }
   ],
   "source": [
    "# Modelo con variables normalizadas\n",
    "\n",
    "# definición del modelo\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=39)\n",
    "\n",
    "# entrenamiento del model\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# evaluación de desempeño\n",
    "print('Segmento de entrenamiento')\n",
    "pred = rf.predict_proba(X_train_scaled)\n",
    "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Segmento de prueba')\n",
    "pred = rf.predict_proba(X_test_scaled)\n",
    "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de esperarse, el desempeño de Random Forest fue el mismo independientemente de la normalización de las variables. Este modelo en particular, esta sobre-ajustando el segemento de entrenamiento. Por lo tanto es necesario ajustar los parámetros del modelo para mejorar la generalización, sin embargo esto esta por fuera del alcance de este demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmento de entrenamiento\n",
      "AdaBoost roc-auc: 0.7970629821021541\n",
      "Test set\n",
      "AdaBoost roc-auc: 0.7473867595818815\n"
     ]
    }
   ],
   "source": [
    "# train adaboost on non-scaled features\n",
    "\n",
    "# call the model\n",
    "ada = AdaBoostClassifier(n_estimators=200, random_state=44)\n",
    "\n",
    "# entrenamiento del model\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# evaluación de desempeño\n",
    "print('Segmento de entrenamiento')\n",
    "pred = ada.predict_proba(X_train)\n",
    "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = ada.predict_proba(X_test)\n",
    "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmento de entrenamiento\n",
      "AdaBoost roc-auc: 0.7970629821021541\n",
      "Test set\n",
      "AdaBoost roc-auc: 0.7475250262706707\n"
     ]
    }
   ],
   "source": [
    "# Modelo con variables normalizadas\n",
    "\n",
    "# definición del modelo\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=200, random_state=44)\n",
    "\n",
    "# entrenamiento del model\n",
    "ada.fit(X_train_scaled, y_train)\n",
    "\n",
    "# evaluación de desempeño\n",
    "print('Segmento de entrenamiento')\n",
    "pred = ada.predict_proba(X_train_scaled)\n",
    "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = ada.predict_proba(X_test_scaled)\n",
    "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de esperarse,  el desempeño de AdaBoost no cambio con la  normalización de las variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Eso es todo por este demo. Esperamos que lo hayan disfrutado y nos vemos en el siguiente.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feml",
   "language": "python",
   "name": "feml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
